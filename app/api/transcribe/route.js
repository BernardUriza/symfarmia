import { pipeline } from '@xenova/transformers';
import { NextRequest, NextResponse } from 'next/server';

// Singleton para evitar cargar el modelo en cada request
let whisperPipeline = null;

async function initializeWhisper() {
  if (!whisperPipeline) {
    console.log('üî• Cargando Whisper model...');
    whisperPipeline = await pipeline(
      'automatic-speech-recognition',
      'Xenova/whisper-small', // Modelo peque√±o para latencia
      {
        revision: 'main',
        quantized: true // Reduce RAM usage
      }
    );
    console.log('‚úÖ Whisper model cargado');
  }
  return whisperPipeline;
}

export async function POST(request) {
  try {
    const formData = await request.formData();
    const audioFile = formData.get('audio');
    
    if (!audioFile) {
      return NextResponse.json(
        { success: false, error: 'No se recibi√≥ archivo de audio' },
        { status: 400 }
      );
    }

    // Convertir audio a ArrayBuffer
    const arrayBuffer = await audioFile.arrayBuffer();
    
    // Inicializar Whisper
    const whisper = await initializeWhisper();
    
    // Transcribir
    console.log('üéôÔ∏è Iniciando transcripci√≥n...');
    const result = await whisper(arrayBuffer, {
      language: 'spanish',
      task: 'transcribe',
      return_timestamps: false,
      chunk_length_s: 30,
      stride_length_s: 5
    });

    console.log('‚úÖ Transcripci√≥n completada');
    
    return NextResponse.json({
      success: true,
      data: {
        text: result.text || '',
        confidence: 0.9, // Whisper no devuelve confidence scores
        language: 'es',
        processingTime: Date.now()
      }
    });

  } catch (error) {
    console.error('‚ùå Error en transcripci√≥n:', error);
    return NextResponse.json(
      { success: false, error: error.message },
      { status: 500 }
    );
  }
}
