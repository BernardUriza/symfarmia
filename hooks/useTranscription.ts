'use client';

import { useState, useCallback, useRef, useEffect } from 'react';
import { useTranslation } from '../app/providers/I18nProvider';

// ğŸ“Š Types que ConversationCapture necesita
export interface TranscriptionResult {
  text: string;
  confidence: number;
  medicalTerms: string[];
  processingTime: number;
}

export type TranscriptionStatus = 'idle' | 'recording' | 'processing' | 'completed' | 'error';

export interface UseTranscriptionReturn {
  // Estados que ConversationCapture consume
  transcription: TranscriptionResult | null;
  status: TranscriptionStatus;
  isRecording: boolean;
  error: string | null;
  engineStatus: 'ready' | 'loading' | 'error' | 'fallback';
  audioLevel: number;
  recordingTime: number;
  
  // MÃ©todos que ConversationCapture necesita
  startTranscription: () => Promise<boolean>;
  stopTranscription: () => Promise<boolean>;
  resetTranscription: () => void;
}

export function useTranscription(options = {}): UseTranscriptionReturn {
  const { t } = useTranslation();
  
  // Estados internos
  const [transcription, setTranscription] = useState<TranscriptionResult | null>(null);
  const [status, setStatus] = useState<TranscriptionStatus>('idle');
  const [isRecording, setIsRecording] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [engineStatus, setEngineStatus] = useState<'ready' | 'loading' | 'error' | 'fallback'>('ready');
  const [audioLevel, setAudioLevel] = useState(0);
  const [recordingTime, setRecordingTime] = useState(0);
  
  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const isRecordingRef = useRef<boolean>(false); // ğŸ”§ Ref para estado de grabaciÃ³n en tiempo real
  const animationFrameRef = useRef<number | null>(null); // ğŸ”§ Para cancelar el loop de monitoreo
  
  // Mantener ref sincronizada con estado
  useEffect(() => {
    console.log(`ğŸ”„ [useEffect] Actualizando isRecordingRef.current = ${isRecording}`);
    isRecordingRef.current = isRecording;
  }, [isRecording]);
  
  // Timer de grabaciÃ³n
  useEffect(() => {
    if (isRecording) {
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
      }, 1000);
    } else {
      if (timerRef.current) {
        clearInterval(timerRef.current);
        timerRef.current = null;
      }
    }
    
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    };
  }, [isRecording]);
  
  // Inicializar audio level monitoring
  const setupAudioMonitoring = useCallback(async (stream: MediaStream) => {
    console.log('ğŸ§ [setupAudioMonitoring] Iniciando configuraciÃ³n de monitoreo de audio...');
    
    try {
      console.log('ğŸ”Š [setupAudioMonitoring] Creando AudioContext...');
      audioContextRef.current = new AudioContext();
      console.log(`âœ… [setupAudioMonitoring] AudioContext creado - sampleRate: ${audioContextRef.current.sampleRate}Hz`);
      
      console.log('ğŸ“Š [setupAudioMonitoring] Creando AnalyserNode...');
      const analyser = audioContextRef.current.createAnalyser();
      console.log(`âœ… [setupAudioMonitoring] AnalyserNode creado - fftSize: ${analyser.fftSize}`);
      
      console.log('ğŸ¤ [setupAudioMonitoring] Creando MediaStreamSource...');
      const source = audioContextRef.current.createMediaStreamSource(stream);
      console.log('âœ… [setupAudioMonitoring] MediaStreamSource creado');
      
      analyser.fftSize = 256;
      console.log(`ğŸ“ [setupAudioMonitoring] FFT Size configurado a: ${analyser.fftSize}`);
      console.log(`ğŸ“Š [setupAudioMonitoring] Frequency bin count: ${analyser.frequencyBinCount}`);
      
      source.connect(analyser);
      console.log('ğŸ”— [setupAudioMonitoring] Source conectado a analyser');
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      console.log(`ğŸ“¦ [setupAudioMonitoring] Array de datos creado con ${dataArray.length} elementos`);
      
      let frameCount = 0;
      const updateAudioLevel = () => {
        if (!isRecordingRef.current) {
          console.log('â¹ï¸ [updateAudioLevel] Deteniendo monitoreo - no estÃ¡ grabando');
          console.log(`ğŸ“Š [updateAudioLevel] isRecordingRef.current = ${isRecordingRef.current}`);
          setAudioLevel(0); // ğŸ”„ Resetear nivel de audio al detener
          return;
        }
        
        analyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        setAudioLevel(average);
        
        // Log cada 30 frames (aproximadamente cada segundo a 30fps)
        if (frameCount % 30 === 0) {
          console.log(`ğŸ“ˆ [updateAudioLevel] Frame ${frameCount} - Nivel de audio: ${average.toFixed(1)}/255 (${((average/255)*100).toFixed(1)}%)`);
          console.log(`ğŸ”„ [updateAudioLevel] isRecordingRef.current = ${isRecordingRef.current}`);
        }
        frameCount++;
        
        // ğŸ”„ Guardar el ID del frame para poder cancelarlo
        animationFrameRef.current = requestAnimationFrame(updateAudioLevel);
      };
      
      console.log('â–¶ï¸ [setupAudioMonitoring] Iniciando loop de monitoreo...');
      console.log(`ğŸ“Š [setupAudioMonitoring] Estado inicial isRecordingRef.current = ${isRecordingRef.current}`);
      updateAudioLevel();
      console.log('âœ… [setupAudioMonitoring] Monitoreo de audio configurado exitosamente');
      
    } catch (error) {
      console.error('ğŸ’¥ [setupAudioMonitoring] Error al configurar monitoreo de audio:', error);
      console.error(`ğŸ” [setupAudioMonitoring] Tipo de error: ${error instanceof Error ? error.constructor.name : typeof error}`);
      console.error(`ğŸ“ [setupAudioMonitoring] Mensaje: ${error instanceof Error ? error.message : String(error)}`);
    }
  }, []); // ğŸ”§ Sin dependencia de isRecording - usamos ref
  
  // Start transcription
  const startTranscription = useCallback(async (): Promise<boolean> => {
    console.log('ğŸš€ [startTranscription] Iniciando funciÃ³n de transcripciÃ³n...');
    
    try {
      console.log('ğŸ§¹ [startTranscription] Limpiando estados previos...');
      setError(null);
      setStatus('recording');
      setEngineStatus('ready');
      setRecordingTime(0);
      console.log('âœ… [startTranscription] Estados inicializados correctamente');
      
      // Verificar permisos de micrÃ³fono
      console.log('ğŸ¤ [startTranscription] Solicitando permisos de micrÃ³fono...');
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log('âœ… [startTranscription] Permisos de micrÃ³fono concedidos');
      console.log(`ğŸ“Š [startTranscription] Stream activo con ${stream.getTracks().length} tracks`);
      
      // Setup audio monitoring
      console.log('ğŸ“ˆ [startTranscription] Configurando monitoreo de audio...');
      await setupAudioMonitoring(stream);
      console.log('âœ… [startTranscription] Monitoreo de audio configurado');
      
      // Setup MediaRecorder
      console.log('ğŸ™ï¸ [startTranscription] Creando MediaRecorder...');
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
        ? 'audio/webm;codecs=opus' 
        : 'audio/webm';
      console.log(`ğŸ“¹ [startTranscription] Usando mimeType: ${mimeType}`);
      
      const mediaRecorder = new MediaRecorder(stream, { mimeType });
      mediaRecorderRef.current = mediaRecorder;
      console.log('âœ… [startTranscription] MediaRecorder creado exitosamente');
      
      const audioChunks: Blob[] = [];
      console.log('ğŸ—‚ï¸ [startTranscription] Array de chunks de audio inicializado');
      
      mediaRecorder.ondataavailable = (event) => {
        console.log(`ğŸ“¦ [ondataavailable] Chunk recibido: ${event.data.size} bytes`);
        audioChunks.push(event.data);
        console.log(`ğŸ“Š [ondataavailable] Total chunks: ${audioChunks.length}`);
      };
      
      mediaRecorder.onstop = async () => {
        console.log('ğŸ›‘ [onstop] MediaRecorder detenido, procesando audio...');
        setStatus('processing');
        
        try {
          // Crear blob de audio
          console.log('ğŸµ [onstop] Creando blob de audio...');
          console.log(`ğŸ“Š [onstop] Total chunks a combinar: ${audioChunks.length}`);
          const totalSize = audioChunks.reduce((acc, chunk) => acc + chunk.size, 0);
          console.log(`ğŸ“ [onstop] TamaÃ±o total del audio: ${totalSize} bytes (${(totalSize / 1024 / 1024).toFixed(2)} MB)`);
          
          const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
          console.log(`âœ… [onstop] Blob creado: ${audioBlob.size} bytes, tipo: ${audioBlob.type}`);
          
          // Enviar al endpoint /api/transcription
          console.log('ğŸ“¤ [onstop] Preparando FormData para envÃ­o...');
          const formData = new FormData();
          formData.append('audio', audioBlob, 'recording.webm');
          console.log('âœ… [onstop] FormData preparado con archivo de audio');
          
          console.log('ğŸŒ [onstop] Enviando audio a /api/transcription...');
          const startTime = Date.now();
          const response = await fetch('/api/transcription', {
            method: 'POST',
            body: formData
          });
          const fetchTime = Date.now() - startTime;
          console.log(`â±ï¸ [onstop] Respuesta recibida en ${fetchTime}ms`);
          console.log(`ğŸ“¡ [onstop] Status HTTP: ${response.status} ${response.statusText}`);
          
          if (!response.ok) {
            console.error(`âŒ [onstop] Error HTTP: ${response.status} ${response.statusText}`);
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
          }
          
          console.log('ğŸ“„ [onstop] Parseando respuesta JSON...');
          const result = await response.json();
          console.log('âœ… [onstop] Respuesta parseada:', result);
          
          if (result.success) {
            console.log('ğŸ‰ [onstop] TranscripciÃ³n exitosa!');
            console.log(`ğŸ“ [onstop] Texto transcrito: "${result.transcript?.substring(0, 50)}..."`);
            console.log(`ğŸ”¢ [onstop] Confianza: ${result.confidence}`);
            console.log(`â±ï¸ [onstop] Tiempo de procesamiento: ${result.processing_time_ms}ms`);
            
            const medicalTerms = extractMedicalTerms(result.transcript || '');
            console.log(`ğŸ¥ [onstop] TÃ©rminos mÃ©dicos encontrados: ${medicalTerms.length}`);
            
            setTranscription({
              text: result.transcript || '',
              confidence: result.confidence || 0,
              medicalTerms: medicalTerms,
              processingTime: result.processing_time_ms || 0
            });
            setStatus('completed');
            console.log('âœ… [onstop] Estado de transcripciÃ³n actualizado a "completed"');
          } else {
            console.error('âŒ [onstop] TranscripciÃ³n fallÃ³:', result.error);
            throw new Error(result.error || 'Transcription failed');
          }
          
        } catch (error) {
          console.error('ğŸ’¥ [onstop] Error en procesamiento de transcripciÃ³n:', error);
          setError(error instanceof Error ? error.message : 'Transcription failed');
          setStatus('error');
          setEngineStatus('error');
          console.log('âŒ [onstop] Estados actualizados a error');
        }
        
        // Cleanup
        console.log('ğŸ§¹ [onstop] Iniciando limpieza...');
        stream.getTracks().forEach((track, index) => {
          console.log(`ğŸ”Œ [onstop] Deteniendo track ${index}: ${track.kind}`);
          track.stop();
        });
        setAudioLevel(0);
        console.log('âœ… [onstop] Limpieza completada');
      };
      
      // Start recording
      console.log('â–¶ï¸ [startTranscription] Iniciando grabaciÃ³n...');
      mediaRecorder.start(1000); // Chunk every second
      console.log('âœ… [startTranscription] MediaRecorder.start() llamado con chunks de 1 segundo');
      
      setIsRecording(true);
      console.log('âœ… [startTranscription] Estado isRecording = true');
      console.log('ğŸ‰ [startTranscription] TranscripciÃ³n iniciada exitosamente!');
      
      return true;
      
    } catch (error) {
      console.error('ğŸ’¥ [startTranscription] Error crÃ­tico al iniciar transcripciÃ³n:', error);
      console.error(`ğŸ” [startTranscription] Tipo de error: ${error instanceof Error ? error.constructor.name : typeof error}`);
      console.error(`ğŸ“ [startTranscription] Mensaje: ${error instanceof Error ? error.message : String(error)}`);
      
      setError(error instanceof Error ? error.message : 'Failed to start recording');
      setStatus('error');
      setEngineStatus('error');
      setIsRecording(false);
      
      console.log('âŒ [startTranscription] Estados actualizados a error, retornando false');
      return false;
    }
  }, [setupAudioMonitoring]);
  
  // Stop transcription
  const stopTranscription = useCallback(async (): Promise<boolean> => {
    console.log('ğŸ›‘ [stopTranscription] Iniciando detenciÃ³n de transcripciÃ³n...');
    
    try {
      console.log(`ğŸ“Š [stopTranscription] Estado actual - isRecording: ${isRecording}`);
      console.log(`ğŸ“Š [stopTranscription] MediaRecorder existe: ${!!mediaRecorderRef.current}`);
      
      if (mediaRecorderRef.current && isRecording) {
        console.log(`ğŸ“Š [stopTranscription] MediaRecorder state: ${mediaRecorderRef.current.state}`);
        console.log('â¹ï¸ [stopTranscription] Llamando mediaRecorder.stop()...');
        
        mediaRecorderRef.current.stop();
        console.log('âœ… [stopTranscription] MediaRecorder.stop() llamado exitosamente');
        
        setIsRecording(false);
        console.log('âœ… [stopTranscription] Estado isRecording = false');
        
        // ğŸ”„ Cancelar el loop de monitoreo de audio
        if (animationFrameRef.current) {
          console.log(`ğŸ›‘ [stopTranscription] Cancelando animation frame: ${animationFrameRef.current}`);
          cancelAnimationFrame(animationFrameRef.current);
          animationFrameRef.current = null;
          console.log('âœ… [stopTranscription] Animation frame cancelado');
        }
        
        // ğŸ”„ Limpiar el contexto de audio
        if (audioContextRef.current) {
          console.log('ğŸ”Œ [stopTranscription] Cerrando AudioContext...');
          audioContextRef.current.close();
          audioContextRef.current = null;
          console.log('âœ… [stopTranscription] AudioContext cerrado');
        }
        
        console.log('ğŸ‰ [stopTranscription] DetenciÃ³n completada exitosamente');
        
        return true;
      }
      
      console.log('âš ï¸ [stopTranscription] No se puede detener - condiciones no cumplidas');
      console.log(`   - mediaRecorderRef.current: ${!!mediaRecorderRef.current}`);
      console.log(`   - isRecording: ${isRecording}`);
      return false;
      
    } catch (error) {
      console.error('ğŸ’¥ [stopTranscription] Error al detener transcripciÃ³n:', error);
      console.error(`ğŸ” [stopTranscription] Tipo de error: ${error instanceof Error ? error.constructor.name : typeof error}`);
      console.error(`ğŸ“ [stopTranscription] Mensaje: ${error instanceof Error ? error.message : String(error)}`);
      
      setError('Failed to stop recording');
      console.log('âŒ [stopTranscription] Error guardado en estado');
      return false;
    }
  }, [isRecording]);
  
  // Reset transcription
  const resetTranscription = useCallback(() => {
    console.log('ğŸ”„ [resetTranscription] Reiniciando estados de transcripciÃ³n...');
    
    setTranscription(null);
    console.log('âœ… [resetTranscription] TranscripciÃ³n = null');
    
    setStatus('idle');
    console.log('âœ… [resetTranscription] Status = idle');
    
    setError(null);
    console.log('âœ… [resetTranscription] Error = null');
    
    setRecordingTime(0);
    console.log('âœ… [resetTranscription] Tiempo de grabaciÃ³n = 0');
    
    setAudioLevel(0);
    console.log('âœ… [resetTranscription] Nivel de audio = 0');
    
    setEngineStatus('ready');
    console.log('âœ… [resetTranscription] Estado del motor = ready');
    
    console.log('ğŸ‰ [resetTranscription] Reinicio completado - todos los estados limpiados');
  }, []);
  
  return {
    transcription,
    status,
    isRecording,
    error,
    engineStatus,
    audioLevel,
    recordingTime,
    startTranscription,
    stopTranscription,
    resetTranscription
  };
}

// Utility function to extract medical terms
function extractMedicalTerms(text: string): string[] {
  const medicalTerms = [
    'dolor', 'fiebre', 'presiÃ³n', 'sangre', 'corazÃ³n', 'pulmÃ³n', 
    'respiraciÃ³n', 'sÃ­ntoma', 'diagnÃ³stico', 'tratamiento', 
    'medicamento', 'alergia', 'diabetes', 'hipertensiÃ³n', 'cefalea'
  ];
  
  const words = text.toLowerCase().split(/\s+/);
  return medicalTerms.filter(term => 
    words.some(word => word.includes(term))
  );
}

export default useTranscription;