'use client';

import { useState, useCallback, useRef, useEffect } from 'react';
import { useTranslation } from '../app/providers/I18nProvider';

// üìä Types que ConversationCapture necesita
export interface TranscriptionResult {
  text: string;
  confidence: number;
  medicalTerms: string[];
  processingTime: number;
}

export type TranscriptionStatus = 'idle' | 'recording' | 'processing' | 'completed' | 'error';

export interface UseTranscriptionReturn {
  // Estados que ConversationCapture consume
  transcription: TranscriptionResult | null;
  status: TranscriptionStatus;
  isRecording: boolean;
  error: string | null;
  engineStatus: 'ready' | 'loading' | 'error' | 'fallback';
  audioLevel: number;
  recordingTime: number;
  
  // M√©todos que ConversationCapture necesita
  startTranscription: () => Promise<boolean>;
  stopTranscription: () => Promise<boolean>;
  resetTranscription: () => void;
}

export function useTranscription(options = {}): UseTranscriptionReturn {
  const { t } = useTranslation();
  
  // Estados internos
  const [transcription, setTranscription] = useState<TranscriptionResult | null>(null);
  const [status, setStatus] = useState<TranscriptionStatus>('idle');
  const [isRecording, setIsRecording] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [engineStatus, setEngineStatus] = useState<'ready' | 'loading' | 'error' | 'fallback'>('ready');
  const [audioLevel, setAudioLevel] = useState(0);
  const [recordingTime, setRecordingTime] = useState(0);
  
  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  
  // Timer de grabaci√≥n
  useEffect(() => {
    if (isRecording) {
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
      }, 1000);
    } else {
      if (timerRef.current) {
        clearInterval(timerRef.current);
        timerRef.current = null;
      }
    }
    
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    };
  }, [isRecording]);
  
  // Inicializar audio level monitoring
  const setupAudioMonitoring = useCallback(async (stream: MediaStream) => {
    console.log('üéß [setupAudioMonitoring] Iniciando configuraci√≥n de monitoreo de audio...');
    
    try {
      console.log('üîä [setupAudioMonitoring] Creando AudioContext...');
      audioContextRef.current = new AudioContext();
      console.log(`‚úÖ [setupAudioMonitoring] AudioContext creado - sampleRate: ${audioContextRef.current.sampleRate}Hz`);
      
      console.log('üìä [setupAudioMonitoring] Creando AnalyserNode...');
      const analyser = audioContextRef.current.createAnalyser();
      console.log(`‚úÖ [setupAudioMonitoring] AnalyserNode creado - fftSize: ${analyser.fftSize}`);
      
      console.log('üé§ [setupAudioMonitoring] Creando MediaStreamSource...');
      const source = audioContextRef.current.createMediaStreamSource(stream);
      console.log('‚úÖ [setupAudioMonitoring] MediaStreamSource creado');
      
      analyser.fftSize = 256;
      console.log(`üìè [setupAudioMonitoring] FFT Size configurado a: ${analyser.fftSize}`);
      console.log(`üìä [setupAudioMonitoring] Frequency bin count: ${analyser.frequencyBinCount}`);
      
      source.connect(analyser);
      console.log('üîó [setupAudioMonitoring] Source conectado a analyser');
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      console.log(`üì¶ [setupAudioMonitoring] Array de datos creado con ${dataArray.length} elementos`);
      
      let frameCount = 0;
      const updateAudioLevel = () => {
        if (!isRecording) {
          console.log('‚èπÔ∏è [updateAudioLevel] Deteniendo monitoreo - no est√° grabando');
          return;
        }
        
        analyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        setAudioLevel(average);
        
        // Log cada 30 frames (aproximadamente cada segundo a 30fps)
        if (frameCount % 30 === 0) {
          console.log(`üìà [updateAudioLevel] Nivel de audio: ${average.toFixed(1)}/255 (${((average/255)*100).toFixed(1)}%)`);
        }
        frameCount++;
        
        requestAnimationFrame(updateAudioLevel);
      };
      
      console.log('‚ñ∂Ô∏è [setupAudioMonitoring] Iniciando loop de monitoreo...');
      updateAudioLevel();
      console.log('‚úÖ [setupAudioMonitoring] Monitoreo de audio configurado exitosamente');
      
    } catch (error) {
      console.error('üí• [setupAudioMonitoring] Error al configurar monitoreo de audio:', error);
      console.error(`üîç [setupAudioMonitoring] Tipo de error: ${error instanceof Error ? error.constructor.name : typeof error}`);
      console.error(`üìù [setupAudioMonitoring] Mensaje: ${error instanceof Error ? error.message : String(error)}`);
    }
  }, [isRecording]);
  
  // Start transcription
  const startTranscription = useCallback(async (): Promise<boolean> => {
    console.log('üöÄ [startTranscription] Iniciando funci√≥n de transcripci√≥n...');
    
    try {
      console.log('üßπ [startTranscription] Limpiando estados previos...');
      setError(null);
      setStatus('recording');
      setEngineStatus('ready');
      setRecordingTime(0);
      console.log('‚úÖ [startTranscription] Estados inicializados correctamente');
      
      // Verificar permisos de micr√≥fono
      console.log('üé§ [startTranscription] Solicitando permisos de micr√≥fono...');
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log('‚úÖ [startTranscription] Permisos de micr√≥fono concedidos');
      console.log(`üìä [startTranscription] Stream activo con ${stream.getTracks().length} tracks`);
      
      // Setup audio monitoring
      console.log('üìà [startTranscription] Configurando monitoreo de audio...');
      await setupAudioMonitoring(stream);
      console.log('‚úÖ [startTranscription] Monitoreo de audio configurado');
      
      // Setup MediaRecorder
      console.log('üéôÔ∏è [startTranscription] Creando MediaRecorder...');
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
        ? 'audio/webm;codecs=opus' 
        : 'audio/webm';
      console.log(`üìπ [startTranscription] Usando mimeType: ${mimeType}`);
      
      const mediaRecorder = new MediaRecorder(stream, { mimeType });
      mediaRecorderRef.current = mediaRecorder;
      console.log('‚úÖ [startTranscription] MediaRecorder creado exitosamente');
      
      const audioChunks: Blob[] = [];
      console.log('üóÇÔ∏è [startTranscription] Array de chunks de audio inicializado');
      
      mediaRecorder.ondataavailable = (event) => {
        console.log(`üì¶ [ondataavailable] Chunk recibido: ${event.data.size} bytes`);
        audioChunks.push(event.data);
        console.log(`üìä [ondataavailable] Total chunks: ${audioChunks.length}`);
      };
      
      mediaRecorder.onstop = async () => {
        console.log('üõë [onstop] MediaRecorder detenido, procesando audio...');
        setStatus('processing');
        
        try {
          // Crear blob de audio
          console.log('üéµ [onstop] Creando blob de audio...');
          console.log(`üìä [onstop] Total chunks a combinar: ${audioChunks.length}`);
          const totalSize = audioChunks.reduce((acc, chunk) => acc + chunk.size, 0);
          console.log(`üìè [onstop] Tama√±o total del audio: ${totalSize} bytes (${(totalSize / 1024 / 1024).toFixed(2)} MB)`);
          
          const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
          console.log(`‚úÖ [onstop] Blob creado: ${audioBlob.size} bytes, tipo: ${audioBlob.type}`);
          
          // Enviar al endpoint /api/transcription
          console.log('üì§ [onstop] Preparando FormData para env√≠o...');
          const formData = new FormData();
          formData.append('audio', audioBlob, 'recording.webm');
          console.log('‚úÖ [onstop] FormData preparado con archivo de audio');
          
          console.log('üåê [onstop] Enviando audio a /api/transcription...');
          const startTime = Date.now();
          const response = await fetch('/api/transcription', {
            method: 'POST',
            body: formData
          });
          const fetchTime = Date.now() - startTime;
          console.log(`‚è±Ô∏è [onstop] Respuesta recibida en ${fetchTime}ms`);
          console.log(`üì° [onstop] Status HTTP: ${response.status} ${response.statusText}`);
          
          if (!response.ok) {
            console.error(`‚ùå [onstop] Error HTTP: ${response.status} ${response.statusText}`);
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
          }
          
          console.log('üìÑ [onstop] Parseando respuesta JSON...');
          const result = await response.json();
          console.log('‚úÖ [onstop] Respuesta parseada:', result);
          
          if (result.success) {
            console.log('üéâ [onstop] Transcripci√≥n exitosa!');
            console.log(`üìù [onstop] Texto transcrito: "${result.transcript?.substring(0, 50)}..."`);
            console.log(`üî¢ [onstop] Confianza: ${result.confidence}`);
            console.log(`‚è±Ô∏è [onstop] Tiempo de procesamiento: ${result.processing_time_ms}ms`);
            
            const medicalTerms = extractMedicalTerms(result.transcript || '');
            console.log(`üè• [onstop] T√©rminos m√©dicos encontrados: ${medicalTerms.length}`);
            
            setTranscription({
              text: result.transcript || '',
              confidence: result.confidence || 0,
              medicalTerms: medicalTerms,
              processingTime: result.processing_time_ms || 0
            });
            setStatus('completed');
            console.log('‚úÖ [onstop] Estado de transcripci√≥n actualizado a "completed"');
          } else {
            console.error('‚ùå [onstop] Transcripci√≥n fall√≥:', result.error);
            throw new Error(result.error || 'Transcription failed');
          }
          
        } catch (error) {
          console.error('üí• [onstop] Error en procesamiento de transcripci√≥n:', error);
          setError(error instanceof Error ? error.message : 'Transcription failed');
          setStatus('error');
          setEngineStatus('error');
          console.log('‚ùå [onstop] Estados actualizados a error');
        }
        
        // Cleanup
        console.log('üßπ [onstop] Iniciando limpieza...');
        stream.getTracks().forEach((track, index) => {
          console.log(`üîå [onstop] Deteniendo track ${index}: ${track.kind}`);
          track.stop();
        });
        setAudioLevel(0);
        console.log('‚úÖ [onstop] Limpieza completada');
      };
      
      // Start recording
      console.log('‚ñ∂Ô∏è [startTranscription] Iniciando grabaci√≥n...');
      mediaRecorder.start(1000); // Chunk every second
      console.log('‚úÖ [startTranscription] MediaRecorder.start() llamado con chunks de 1 segundo');
      
      setIsRecording(true);
      console.log('‚úÖ [startTranscription] Estado isRecording = true');
      console.log('üéâ [startTranscription] Transcripci√≥n iniciada exitosamente!');
      
      return true;
      
    } catch (error) {
      console.error('üí• [startTranscription] Error cr√≠tico al iniciar transcripci√≥n:', error);
      console.error(`üîç [startTranscription] Tipo de error: ${error instanceof Error ? error.constructor.name : typeof error}`);
      console.error(`üìù [startTranscription] Mensaje: ${error instanceof Error ? error.message : String(error)}`);
      
      setError(error instanceof Error ? error.message : 'Failed to start recording');
      setStatus('error');
      setEngineStatus('error');
      setIsRecording(false);
      
      console.log('‚ùå [startTranscription] Estados actualizados a error, retornando false');
      return false;
    }
  }, [setupAudioMonitoring]);
  
  // Stop transcription
  const stopTranscription = useCallback(async (): Promise<boolean> => {
    console.log('üõë [stopTranscription] Iniciando detenci√≥n de transcripci√≥n...');
    
    try {
      console.log(`üìä [stopTranscription] Estado actual - isRecording: ${isRecording}`);
      console.log(`üìä [stopTranscription] MediaRecorder existe: ${!!mediaRecorderRef.current}`);
      
      if (mediaRecorderRef.current && isRecording) {
        console.log(`üìä [stopTranscription] MediaRecorder state: ${mediaRecorderRef.current.state}`);
        console.log('‚èπÔ∏è [stopTranscription] Llamando mediaRecorder.stop()...');
        
        mediaRecorderRef.current.stop();
        console.log('‚úÖ [stopTranscription] MediaRecorder.stop() llamado exitosamente');
        
        setIsRecording(false);
        console.log('‚úÖ [stopTranscription] Estado isRecording = false');
        console.log('üéâ [stopTranscription] Detenci√≥n completada exitosamente');
        
        return true;
      }
      
      console.log('‚ö†Ô∏è [stopTranscription] No se puede detener - condiciones no cumplidas');
      console.log(`   - mediaRecorderRef.current: ${!!mediaRecorderRef.current}`);
      console.log(`   - isRecording: ${isRecording}`);
      return false;
      
    } catch (error) {
      console.error('üí• [stopTranscription] Error al detener transcripci√≥n:', error);
      console.error(`üîç [stopTranscription] Tipo de error: ${error instanceof Error ? error.constructor.name : typeof error}`);
      console.error(`üìù [stopTranscription] Mensaje: ${error instanceof Error ? error.message : String(error)}`);
      
      setError('Failed to stop recording');
      console.log('‚ùå [stopTranscription] Error guardado en estado');
      return false;
    }
  }, [isRecording]);
  
  // Reset transcription
  const resetTranscription = useCallback(() => {
    console.log('üîÑ [resetTranscription] Reiniciando estados de transcripci√≥n...');
    
    setTranscription(null);
    console.log('‚úÖ [resetTranscription] Transcripci√≥n = null');
    
    setStatus('idle');
    console.log('‚úÖ [resetTranscription] Status = idle');
    
    setError(null);
    console.log('‚úÖ [resetTranscription] Error = null');
    
    setRecordingTime(0);
    console.log('‚úÖ [resetTranscription] Tiempo de grabaci√≥n = 0');
    
    setAudioLevel(0);
    console.log('‚úÖ [resetTranscription] Nivel de audio = 0');
    
    setEngineStatus('ready');
    console.log('‚úÖ [resetTranscription] Estado del motor = ready');
    
    console.log('üéâ [resetTranscription] Reinicio completado - todos los estados limpiados');
  }, []);
  
  return {
    transcription,
    status,
    isRecording,
    error,
    engineStatus,
    audioLevel,
    recordingTime,
    startTranscription,
    stopTranscription,
    resetTranscription
  };
}

// Utility function to extract medical terms
function extractMedicalTerms(text: string): string[] {
  const medicalTerms = [
    'dolor', 'fiebre', 'presi√≥n', 'sangre', 'coraz√≥n', 'pulm√≥n', 
    'respiraci√≥n', 's√≠ntoma', 'diagn√≥stico', 'tratamiento', 
    'medicamento', 'alergia', 'diabetes', 'hipertensi√≥n', 'cefalea'
  ];
  
  const words = text.toLowerCase().split(/\s+/);
  return medicalTerms.filter(term => 
    words.some(word => word.includes(term))
  );
}

export default useTranscription;